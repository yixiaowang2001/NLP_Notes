# Week 2

# 1 Transformers vs RNNs

- RNNs -> no parallel computing

<p align="center">
  <img src="../res/img/img45.png" width="500"/>
  <img src="../res/img/img46.png" width="500"/>
</p>

- RNNs vs Transformers

<p align="center">
  <img src="../res/img/img47.png" width="600"/>
</p>

# 2 Transformers

## 2.1 Overview

- Multi-head attention model

<p align="center">
  <img src="../res/img/img48.png" width="600"/>
</p>

- The encoder & decoder

<p align="center">
  <img src="../res/img/img49.png" width="500"/>
  <img src="../res/img/img50.png" width="500"/>
</p>

- Positional encoding

<p align="center">
  <img src="../res/img/img51.png" width="600"/>
</p>

- The transformer

<p align="center">
  <img src="../res/img/img52.png" width="600"/>
</p>

- Summary

<p align="center">
  <img src="../res/img/img53.png" width="600"/>
</p>

## 2.2 Applications

- Applications

<p align="center">
  <img src="../res/img/img54.png" width="600"/>
</p>

- NLP models

<p align="center">
  <img src="../res/img/img55.png" width="600"/>
</p>

- Breif introduction of T5 (a multi-task transformer)

<p align="center">
  <img src="../res/img/img56.png" width="500"/>
  <img src="../res/img/img57.png" width="500"/>
</p>

# 3 Attention

## 3.1 Scaled Dot Product Attention

- Recap

<p align="center">
  <img src="../res/img/img58.png" width="600"/>
</p>

- Queries, keys, and values

<p align="center">
  <img src="../res/img/img59.png" width="600"/>
</p>

- Attention Math

<p align="center">
  <img src="../res/img/img60.png" width="600"/>
</p>

## 3.2 Masked Self Attention

- Encoder-decoder attention

<p align="center">
  <img src="../res/img/img61.png" width="600"/>
</p>

- Self attention

<p align="center">
  <img src="../res/img/img62.png" width="600"/>
</p>

- Masked self-attention math

<p align="center">
  <img src="../res/img/img63.png" width="600"/>
</p>

## 3.3 Multi-head Attention

- Overview

<p align="center">
  <img src="../res/img/img64.png" width="500"/>
  <img src="../res/img/img65.png" width="500"/>
</p>

- Multi-head attention structure

<p align="center">
  <img src="../res/img/img66.png" width="600"/>
</p>

# 4 Transformer Decoder

- General

<p align="center">
  <img src="../res/img/img67.png" width="500"/>
  <img src="../res/img/img68.png" width="500"/>
</p>

- Decoder block

<p align="center">
  <img src="../res/img/img69.png" width="600"/>
</p>

- Feed forward layer

<p align="center">
  <img src="../res/img/img70.png" width="600"/>
</p>

# 5 Transformer Summarizer

- Data processing

<p align="center">
  <img src="../res/img/img71.png" width="600"/>
</p>

- Cost Function

<p align="center">
  <img src="../res/img/img72.png" width="600"/>
</p>

- Inference

<p align="center">
  <img src="../res/img/img73.png" width="600"/>
</p>
