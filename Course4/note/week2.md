# Week 2

# 1 Transformers vs RNNs

- RNNs -> no parallel computing

<p align="center">
  <img src="../res/img/img45.png" width="500"/>
  <img src="../res/img/img46.png" width="500"/>
</p>

- RNNs vs Transformers

<p align="center">
  <img src="../res/img/img47.png" width="600"/>
</p>

# 2 Transformers

## 2.1 Overview

- Multi-head attention model

<p align="center">
  <img src="../res/img/img48.png" width="600"/>
</p>

- The encoder & decoder

<p align="center">
  <img src="../res/img/img49.png" width="500"/>
  <img src="../res/img/img50.png" width="500"/>
</p>

- Positional encoding

<p align="center">
  <img src="../res/img/img51.png" width="600"/>
</p>

- The transformer

<p align="center">
  <img src="../res/img/img52.png" width="600"/>
</p>

- Summary

<p align="center">
  <img src="../res/img/img53.png" width="600"/>
</p>

## 2.2 Applications

- Applications

<p align="center">
  <img src="../res/img/img54.png" width="600"/>
</p>

- NLP models

<p align="center">
  <img src="../res/img/img55.png" width="600"/>
</p>

- Breif introduction of T5 (a multi-task transformer)

<p align="center">
  <img src="../res/img/img56.png" width="500"/>
  <img src="../res/img/img57.png" width="500"/>
</p>

## 2.3 Scaled Dot Product Attention

- Recap

<p align="center">
  <img src="../res/img/img58.png" width="600"/>
</p>

- Queries, keys, and values

<p align="center">
  <img src="../res/img/img59.png" width="600"/>
</p>

- Attention Math

<p align="center">
  <img src="../res/img/img60.png" width="600"/>
</p>
