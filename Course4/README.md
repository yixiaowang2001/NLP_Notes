# Natural Language Processing with Attention Models

Coursera course from DeepLearning.AI ([link](https://www.coursera.org/learn/attention-models-in-nlp))

<div align="center">

| **Week** |                                        **Note**                                         |                                                                                                                                                                                                                                                                                                           **Code**                                                                                                                                                                                                                                                                                                           |              **Status**              |                                                  **Keywords**                                                  |
| :------: | :-------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :----------------------------------: | :------------------------------------------------------------------------------------------------------------: |
|  week1   | [week1.md](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/note/week1.md) | [W1L1](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/lab/W1/C4_W1_Ungraded_Lab_1_Basic_Attention.ipynb) / [W1L2](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/lab/W1/C4_W1_Ungraded_Lab_2_QKV_Attention.ipynb) / [W1L3](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/lab/W1/C4_W1_Ungraded_Lab_3_Bleu_Score.ipynb) / [W1L4](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/lab/W1/C4_W1_Ungraded_Lab_4_Stack_Semantics.ipynb) / [W1A1](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/hw/W1/C4_W1_Assignment.ipynb) | ![pb1](https://progress-bar.dev/100) | Seq2seq with Attention, Neural Machine Translation, BLEU Score, ROUGE-N Score, Beam Search, Minimum Bayes Risk |
|  week2   | [week2.md](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/note/week2.md) |                                                                                                                             [W2L1](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/lab/W2/C4_W2_Ungraded_Lab_1_Attention.ipynb) / [W2L2](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/lab/W2/C4_W2_Ungraded_Lab_2_Transformer_Decoder.ipynb) / [W2A1](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/hw/W2/C4_W2_Assignment.ipynb)                                                                                                                              | ![pb2](https://progress-bar.dev/100) |                                      Transformers, Transformer Summarizer                                      |
|  week3   | [week3.md](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/note/week3.md) |                                         [W3L1](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/lab/W3/C4_W3_SentencePiece_and_BPE.ipynb) / [W3L2](https://drive.google.com/file/d/1O4LvdhHw6Zx7Kd43HK-p5a1rtsHUEia5/view) / [W3L3](https://drive.google.com/file/d/1P8COnbYLphJNaW3v8wS1AwpahnV-653A/view) / [W3L4](https://drive.google.com/file/d/1Hz15z7TGxx-5MYizMfCGD5CcRt2ZDbL5/view) / [W3L5](https://drive.google.com/file/d/1hc7PaXjuuMS0likb0etEHY0ryAzsqAZR/view) / [W3A1](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/hw/W3/)                                          | ![pb3](https://progress-bar.dev/100) |                          Question Answering, Transfer Learning, BERT, T5, HuggingFace                          |
|  week4   | [week4.md](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/note/week4.md) |                                                                                                                                  [W4L1](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/lab/W4/C4_W4_Ungraded_Lab_1_Reformer_LSH.ipynb) / [W4L2](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/lab/W4/C4_W4_Ungraded_Lab_2_Revnet.ipynb) / [W4A1](https://github.com/yixiaowang2001/NLP_Notes/blob/main/Course4/code/hw/W4/C4_W4_Assignment.ipynb)                                                                                                                                   |  ![pb4](https://progress-bar.dev/0)  |                                                       /                                                        |

</div>
